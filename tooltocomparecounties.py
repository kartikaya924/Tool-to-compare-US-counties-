# -*- coding: utf-8 -*-
"""Copy of Synthetic_Control_County_Level.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RX0m-1nXf3FalRuMZDBJQJzseG1L5dvC

# Importing Necessary Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns

import requests
from bs4 import BeautifulSoup

"""# New York Times Data"""

nyTimesData = pd.read_csv("https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv")#.set_index(["date", "state", "county"])

nyTimesDataAdded2 = pd.DataFrame()

for state_name, state in nyTimesData.groupby("state"):
  nyTimesDataAdded = pd.DataFrame()
  for county_name, county in state.groupby("county"):
    county["new cases"] = county["cases"].diff()
    county["new deaths"] = county["deaths"].diff()
    nyTimesDataAdded = pd.concat([nyTimesDataAdded, county])
  nyTimesDataAdded2 = pd.concat([nyTimesDataAdded2, nyTimesDataAdded])
  print(state_name)

nyTimesData = nyTimesDataAdded2.reset_index(drop = True).set_index(["date", "county", "state"]).drop("fips", axis = 1)
nyTimesData

nyTimesData.xs("Kansas", level = "state").xs("Douglas", level = "county")

"""#Census Information"""

URL = "https://www.census.gov/data/tables/time-series/demo/popest/2010s-counties-detail.html"
a = BeautifulSoup(requests.get(URL).text, "html.parser")
[i for i in [i for i in [i.get_attribute_list("class")[0] for i in a.find_all("div")] if i!=None] if i.startswith("state")]

age_html = [i for i in [i for i in  [i for i in [i for i in a.find_all("div") if len(i.get_attribute_list("class"))>0 ]]  if not( i.get_attribute_list("class")[0] in [None, ""])] if i.get_attribute_list("class")[0].startswith("state")][0]
age_databases = pd.DataFrame( data = [[i.get_text().strip(), i.get_attribute_list("href")[0] ] for i in age_html.find_all("a")], columns = ["State", "Link"]).set_index("State")
age_databases

main_df = pd.DataFrame()
state = age_databases.index[0]
for state in age_databases.index:
  try:
    x = pd.read_csv("https:" + age_databases["Link"].loc[state]).drop(["SUMLEV", "STNAME", "COUNTY"], axis = 1)
    x = x.groupby("CTYNAME").apply(np.mean).drop(["YEAR"], axis = 1).reset_index()
    x["STATE"] = state
    #x = x.set_index(["STATE", "CTYNAME"]).drop([i for i in x.columns if ("MALE" in i) or ("FEM" in i)], axis = 1)

    land_area = pd.read_html("https://en.wikipedia.org/wiki/List_of_counties_in_"+state.replace(" ", "_"))
    land_area = [i for i in land_area if len(i.columns) > 8][0]
    division = land_area.columns[0]
    land_area = land_area.drop(["Map"], axis = 1, errors = "ignore").set_index(division)
    land_area = land_area[ [ land_area.columns[-1]] ].applymap(lambda area: float("".join((area.split()[0]).split(",")))  ).rename(columns = {land_area.columns[-1]:"Land Area"}).applymap(round).reset_index()
    x = x.merge(land_area, left_on = "CTYNAME", right_on = division).drop([division], axis = 1)
    x["Population Density"] = x["POPESTIMATE"] / x["Land Area"]
    main_df = pd.concat([main_df, x])
    print(state)
  except:
    print(state + " (Could not add)")

main_df = main_df.rename(columns = {"CTYNAME":"County", "STATE":"State"}).set_index(["State", "County"])
full_information = main_df
main_df = main_df[["POPESTIMATE", "MEDIAN_AGE_TOT", "Land Area", "Population Density"]].rename(columns = dict(zip ( ["POPESTIMATE", "MEDIAN_AGE_TOT"], 
                                                                                                                    ["Population", "Median Age"]) ) )
main_df.xs("Kansas")

df=main_df.xs("Kansas").drop('Land Area', axis=1)

df

from google.colab import files
df.to_csv('test2.csv') 
files.download('test2.csv')

"""# KMeans Clustering"""

from sklearn.cluster import KMeans as myModel

ST_NAME = "Kansas"

X_raw = main_df.xs(ST_NAME).drop("Land Area", axis = 1).reset_index()
X = X_raw.drop("County", axis = 1)


points = [[], []]
for i in range(10):
  clusters = myModel(n_clusters = i+1).fit(X)
  points[0].append(i+1)
  points[1].append(round(clusters.inertia_, 0))

first = points[1][0]
power = int(np.log10(first))-3
points[1] = list(map(lambda x: x/(10**power), points[1]))
pd.DataFrame(points, index = ["K", "Inertia (scaled down)"]).T.set_index("K")

fig = plt.figure(figsize = (5, 5), facecolor="bisque")
plt.plot(points[0][1:], points[1][1:])
plt.xticks(np.arange(1, 11))
plt.ylabel("Inertia (in thousands)", fontname = "Serif", fontsize = 15)
plt.xlabel("Number of clusters", fontname = "Serif", fontsize = 15)
plt.title("Finding the Optimal Number for K \n in the Kmeans algorithm", fontsize = 20, fontname = "cmtt10")
plt.show()

inertia = pd.DataFrame(points[1], index = points[0], columns = ["Inertia"])
inertia["Second Derivative"] = inertia["Inertia"].diff()
inertia["Second Derivative"][:-1] = inertia["Second Derivative"][1:]
inertia["Second Derivative"] = inertia["Second Derivative"].diff()
inertia["Second Derivative"][:-1] = inertia["Second Derivative"][1:]
inertia["Scaled Second Derivative"] = inertia["Second Derivative"] / inertia["Second Derivative"].iloc[0]
good_cluster_estimate = inertia.query("`Scaled Second Derivative`<=0.01").iloc[0].name
print("\n\n\nLikely number of clusters: {} \n\n(for population size, population density, and median age)\n\n\n".
      format(good_cluster_estimate))

clusters = myModel(n_clusters = 3).fit(X)
clustered_counties = pd.DataFrame(clusters.labels_, columns = ["Category"])
clustered_counties["County"] = X_raw["County"]
clustered_counties = clustered_counties[["County", "Category"]].set_index("County")

pd.DataFrame(clustered_counties.groupby("Category").apply(len), 
             columns = ["Counties per Category"])

rolling_window = 14

counties = clustered_counties.query("Category == 2").index
counties = list(map(lambda county_name: county_name.split(" County")[0], counties))
state_level = nyTimesData.xs(ST_NAME, level = "state").reset_index()

relevant_counties = pd.DataFrame()

for county in counties:
  current_county_info = state_level[state_level["county"] == county]
  
  for field in current_county_info.columns:
    if not(field in ["date", "county", "state"]):
      current_county_info[field + " ({}-day Rolling Average)".format(rolling_window)] = current_county_info[field].rolling(rolling_window).mean()
  relevant_counties = pd.concat([relevant_counties, current_county_info])
  print(county)

relevant_counties = relevant_counties.reset_index(drop = True)
relevant_counties

import matplotlib.dates as mdates

fig = plt.figure(figsize = (60, 10))
ax = sns.lineplot(x = "date", y = "cases (14-day Rolling Average)", hue = "county", data = relevant_counties.sort_values(by = "date"), palette = ["black", "lightcoral", "red", "peru", "darkorange",
                                                                                                                                                  "gold", "yellow", "olivedrab", "lawngreen",
                                                                                                                                                  "mediumturquoise", "dodgerblue", "darkviolet",
                                                                                                                                                  "fuchsia"])
#plt.plot(relevant_counties.sort_values(by = "date")["date"], relevant_counties.sort_values(by = "date")["new cases (14-day Rolling Average)"])
#
fig.autofmt_xdate()
#ax.xaxis.set_major_locator(mdates.WeekdayLocator(interval=1))
#x = plt.xticks()

#plt.xticks(x, rotation=90)
#print(plt.xticks())
#plt.tight_layout()
plt.show()

